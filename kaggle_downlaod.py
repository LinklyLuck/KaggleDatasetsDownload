import os
import subprocess
import zipfile
import time
import csv
import hashlib
import shutil
import re
import random
import unicodedata
from collections import defaultdict

# ================== åŸºç¡€é…ç½®==================
KAGGLE_API_TOKEN = ""
os.environ["KAGGLE_API_TOKEN"] = KAGGLE_API_TOKEN

BASE_DIR = r"D:\kaggle_pool"   # å…¨éƒ¨è¾“å‡ºæ”¾åˆ° D ç›˜è¿™ä¸ªç›®å½•
# ===============================================================

# ===================== é‡‡é›†ç›®æ ‡ä¸è¿‡æ»¤å‚æ•° ======================
TARGET_MAX = 8000
ALLOW_DOWNLOAD_IF_SIZE_UNKNOWN = True

MIN_ROWS = 300
MAX_ROWS = 50000
MIN_COLS = 4

MAX_CSV_PER_DATASET = 5                   # æ¯ä¸ªæ•°æ®é›†æœ€ç»ˆæœ€å¤šè½ç›˜ 5 ä¸ª
MAX_SCAN_CSV_ENTRIES_PER_DATASET = 200    # æ¯ä¸ªæ•°æ®é›†æœ€å¤šæ‰«æå¤šå°‘ä¸ªCSVæ¡ç›®ï¼ˆå¤Ÿç”¨ä¸”å¿«ï¼‰
MAX_DATASET_TOTAL_MB = 2048               # ä¸‹è½½å‰é¢„æ£€æŸ¥ï¼šæ•°æ®é›†æ€»å¤§å° <= 2GB æ‰ä¸‹è½½

SEARCH_KEYWORDS = [
    "csv", "tabular", "dataset",
    "business", "finance", "sales", "marketing",
    "education", "university", "students",
    "sports", "football", "basketball",
    "movies", "film", "imdb",
    "health", "medical",
    "government", "census",
    "technology", "startup",
    "traffic", "transportation",
    "climate", "energy",
    "retail", "consumer",
    "real estate", "housing"
]

PAGES_PER_KEYWORD = 50

# æ¯ä¸ªæ•°æ®é›†ä¹‹é—´çš„åŸºç¡€ä¼‘çœ ï¼ˆä¼šå åŠ éšæœºæŠ–åŠ¨ï¼Œé™ä½è¢«é™æµæ¦‚ç‡ï¼‰
BASE_SLEEP = 0.6
JITTER_SLEEP = (0.0, 0.6)  # éšæœºåŠ  0~0.6 ç§’
# ===============================================================

RAW_DIR = os.path.join(BASE_DIR, "raw_datasets")
CSV_DIR = os.path.join(BASE_DIR, "all_csv")
INDEX_PATH = os.path.join(BASE_DIR, "index.csv")

os.makedirs(RAW_DIR, exist_ok=True)
os.makedirs(CSV_DIR, exist_ok=True)

csv_hashes = set()            # å…¨å±€å»é‡ï¼šå†…å®¹ MD5
downloaded_datasets = set()
index_rows = []


# ------------------------------ æ–‡ä»¶åä¹±ç å¤„ç†ï¼ˆæ–°å¢ï¼‰ ------------------------------
def try_fix_zip_name(name: str) -> str:
    """
    å°è¯•ä¿®å¤ zip å†…éƒ¨æ–‡ä»¶åç¼–ç ï¼ˆä¸ä¿è¯100%ï¼‰ã€‚
    å¸¸è§æƒ…å†µï¼šzipæŒ‰cp437è§£é‡Šï¼Œå®é™…æ˜¯utf-8/gbk/big5ã€‚
    """
    # å¦‚æœæœ¬æ¥å°±æ²¡æœ‰æ›¿æ¢ç¬¦ï¼Œç›´æ¥è¿”å›
    if "ï¿½" not in name:
        return name
    try:
        raw = name.encode("cp437", errors="replace")
        candidates = []
        for enc in ("utf-8", "gbk", "big5"):
            try:
                fixed = raw.decode(enc, errors="replace")
                candidates.append(fixed)
            except Exception:
                pass
        if not candidates:
            return name
        # é€‰æ›¿æ¢ç¬¦æœ€å°‘çš„é‚£ä¸ª
        candidates.sort(key=lambda s: s.count("ï¿½"))
        best = candidates[0]
        # å¦‚æœæ²¡æœ‰æ›´å¥½ï¼Œå°±åˆ«ä¹±æ”¹
        if best.count("ï¿½") >= name.count("ï¿½"):
            return name
        return best
    except Exception:
        return name


def sanitize_filename(name: str, max_len: int = 120) -> str:
    """
    æŠŠä»»æ„å­—ç¬¦ä¸²å˜æˆ Windows å¯è½ç›˜çš„æ–‡ä»¶åï¼š
      - è§„èŒƒåŒ– Unicode
      - å»æ‰ Windows ç¦æ­¢å­—ç¬¦
      - éå¸¸è§„å­—ç¬¦æ›¿æ¢æˆ _
      - å‹ç¼©å¤šä½™ç©ºæ ¼/ä¸‹åˆ’çº¿
      - æˆªæ–­é•¿åº¦
    """
    name = unicodedata.normalize("NFKC", name)

    # å»æ‰ Windows ä¸å…è®¸çš„å­—ç¬¦å’Œæ§åˆ¶å­—ç¬¦
    name = re.sub(r'[<>:"/\\|?*\x00-\x1F]', "_", name)

    # åªä¿ç•™å®‰å…¨å­—ç¬¦é›†åˆï¼Œå…¶ä½™æ›¿æ¢ _
    name = re.sub(r'[^0-9a-zA-Z._\- \u4e00-\u9fff]+', "_", name)

    # å‹ç¼©ç©ºç™½/ä¸‹åˆ’çº¿
    name = re.sub(r"\s+", " ", name).strip()
    name = re.sub(r"_+", "_", name)

    # æˆªæ–­
    if len(name) > max_len:
        base, ext = os.path.splitext(name)
        name = base[: max_len - len(ext)] + ext

    return name or "file"


def safe_output_name(orig_basename: str, md5_hex: str) -> str:
    """
    ç”¨â€œåŸå§‹ basenameï¼ˆå¯èƒ½ä¹±ç ï¼‰+ md5çŸ­åç¼€â€ç”Ÿæˆå®‰å…¨æ–‡ä»¶åï¼Œé¿å…å†²çª/ä¹±ç ã€‚
    """
    base, ext = os.path.splitext(orig_basename)
    ext = ext if ext else ".csv"
    safe_base = sanitize_filename(base)
    suffix = md5_hex[:10]
    return f"{safe_base}_{suffix}{ext}"


# ------------------------------ é€šç”¨å·¥å…·ï¼šé‡è¯•æ‰§è¡Œ ------------------------------
def run_with_retry(cmd, *, retries=3, base_delay=2.0, jitter=1.0, timeout=None,
                   capture_output=False, stdout_to_null=False):
    """
    ç”¨äº kaggle list/files è¿™ç±»â€œå¶å‘ç½‘ç»œå¤±è´¥â€çš„å‘½ä»¤ã€‚
    """
    last = None
    for attempt in range(1, retries + 1):
        try:
            if stdout_to_null:
                result = subprocess.run(
                    cmd,
                    text=True,
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.PIPE,
                    timeout=timeout
                )
            else:
                result = subprocess.run(
                    cmd,
                    text=True,
                    capture_output=capture_output,
                    timeout=timeout
                )

            if result.returncode == 0:
                return result

            last = result
            if attempt < retries:
                delay = base_delay * attempt + random.random() * jitter
                print(f"âš ï¸ å‘½ä»¤å¤±è´¥ï¼Œ{delay:.1f}s åé‡è¯• ({attempt}/{retries})ï¼š{' '.join(cmd)}")
                time.sleep(delay)

        except subprocess.TimeoutExpired as e:
            last = e
            if attempt < retries:
                delay = base_delay * attempt + random.random() * jitter
                print(f"âš ï¸ å‘½ä»¤è¶…æ—¶ï¼Œ{delay:.1f}s åé‡è¯• ({attempt}/{retries})ï¼š{' '.join(cmd)}")
                time.sleep(delay)

    return last


# ------------------------------ Kaggleå‘½ä»¤å°è£… ------------------------------
def kaggle_download(dataset_ref: str) -> bool:
    cmd = ["kaggle", "datasets", "download", "-d", dataset_ref, "-p", RAW_DIR]
    res = run_with_retry(cmd, retries=2, base_delay=3.0, jitter=2.0, timeout=None,
                         capture_output=False, stdout_to_null=True)
    return hasattr(res, "returncode") and res.returncode == 0


def kaggle_list_datasets(keyword: str, page: int):
    cmd = ["kaggle", "datasets", "list", "-s", keyword, "-p", str(page), "-v"]
    res = run_with_retry(cmd, retries=3, base_delay=2.0, jitter=1.5, timeout=90,
                         capture_output=True, stdout_to_null=False)
    if not hasattr(res, "returncode") or res.returncode != 0:
        return None
    return res.stdout


def kaggle_dataset_files(dataset_ref: str):
    cmd = ["kaggle", "datasets", "files", "-d", dataset_ref]
    res = run_with_retry(cmd, retries=3, base_delay=2.0, jitter=1.5, timeout=90,
                         capture_output=True, stdout_to_null=False)
    if not hasattr(res, "returncode") or res.returncode != 0:
        print("âŒ fileså¤±è´¥:", dataset_ref)
        try:
            print(res.stderr)
        except:
            pass
        return None
    return res.stdout


def dataset_total_size_mb_via_metadata(dataset_ref: str) -> float:
    meta_dir = os.path.join(RAW_DIR, "_meta")
    os.makedirs(meta_dir, exist_ok=True)

    for f in os.listdir(meta_dir):
        if f.endswith(".json"):
            try:
                os.remove(os.path.join(meta_dir, f))
            except:
                pass

    cmd = ["kaggle", "datasets", "metadata", "-d", dataset_ref, "-p", meta_dir]
    res = run_with_retry(cmd, retries=3, base_delay=2.0, jitter=1.5, timeout=90,
                         capture_output=True, stdout_to_null=False)
    if not hasattr(res, "returncode") or res.returncode != 0:
        try:
            print("âŒ metadataå¤±è´¥:", dataset_ref)
            print(res.stderr)
        except:
            pass
        return -1.0

    json_files = [x for x in os.listdir(meta_dir) if x.endswith(".json")]
    if not json_files:
        return -1.0

    meta_path = os.path.join(meta_dir, json_files[0])
    try:
        import json
        with open(meta_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        total_bytes = data.get("totalBytes", None)
        if total_bytes is None:
            return -1.0
        return float(total_bytes) / (1024 * 1024)
    except Exception:
        return -1.0


# ------------------------------ é¢„æ£€æŸ¥ï¼šæ•°æ®é›†å¤§å° ------------------------------
def dataset_total_size_mb(dataset_ref: str) -> float:
    mb = dataset_total_size_mb_via_metadata(dataset_ref)
    if mb >= 0:
        return mb

    out = kaggle_dataset_files(dataset_ref)
    if out is None:
        return float("inf")

    total_mb = 0.0
    found = 0
    for line in out.splitlines():
        line = line.strip()
        if not line:
            continue
        if line.lower().startswith("name") or line.startswith("-"):
            continue

        m = re.search(r"(\d+(?:\.\d+)?)\s*(KB|MB|GB)\b\s*$", line, re.IGNORECASE)
        if not m:
            continue

        num = float(m.group(1))
        unit = m.group(2).upper()
        found += 1

        if unit == "KB":
            total_mb += num / 1024
        elif unit == "MB":
            total_mb += num
        elif unit == "GB":
            total_mb += num * 1024

    if found == 0:
        return float("inf")
    return total_mb


# ------------------------------ CSV å¤„ç†ï¼šè¡Œåˆ—/å»é‡/ç´¢å¼• ------------------------------
def file_hash(path: str) -> str:
    h = hashlib.md5()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def file_size_kb(path: str) -> float:
    return round(os.path.getsize(path) / 1024, 2)


def count_rows_cols(path: str):
    with open(path, "r", encoding="utf-8", errors="ignore", newline="") as f:
        r = csv.reader(f)
        header = next(r, [])
        cols = len(header)
        rows = 0
        for _ in r:
            rows += 1
    return rows, cols


def safe_unique_name(desired_name: str) -> str:
    p = os.path.join(CSV_DIR, desired_name)
    if not os.path.exists(p):
        return desired_name
    base, ext = os.path.splitext(desired_name)
    return f"{base}_{time.time_ns()}{ext}"


def name_signature(filename: str) -> str:
    """
    â€œè¡¨åâ€ï¼šæŒ‰æ–‡ä»¶åå½’ä¸€åŒ–ï¼ˆä½†å…ˆ sanitizeï¼Œé¿å…ä¹±ç å¯¼è‡´ä¸ç¨³å®šï¼‰ã€‚
    ä¾‹ï¼štrain_1.csv / train_2.csv / train003.csv => train
    """
    base = sanitize_filename(os.path.basename(filename))
    stem = os.path.splitext(base)[0].strip().lower()
    stem = re.sub(r"[\s_\-]*\(\d+\)$", "", stem)
    stem = re.sub(r"[\s_\-]*\d+$", "", stem)
    stem = " ".join(stem.split())
    return stem or stem


def write_index():
    if not index_rows:
        return
    write_header = not os.path.exists(INDEX_PATH)
    with open(INDEX_PATH, "a", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        if write_header:
            # âœ… æ–°å¢ä¸¤åˆ—ï¼šorig_zip_name / fixed_zip_name
            w.writerow(["filename", "rows", "cols", "size_kb", "md5", "source", "keyword",
                        "name_sig", "orig_zip_name", "fixed_zip_name"])
        w.writerows(index_rows)
    index_rows.clear()


def newest_zip_in_dir(folder: str):
    zips = [f for f in os.listdir(folder) if f.endswith(".zip")]
    if not zips:
        return None
    zips.sort(key=lambda x: os.path.getmtime(os.path.join(folder, x)), reverse=True)
    return os.path.join(folder, zips[0])


def clear_raw_zips():
    for f in os.listdir(RAW_DIR):
        if f.endswith(".zip"):
            try:
                os.remove(os.path.join(RAW_DIR, f))
            except:
                pass


def extract_and_filter(zip_path, dataset_ref, keyword):
    """
    ä»zipé‡ŒæŒ‘CSVï¼š
      - è¡Œåˆ—è¿‡æ»¤
      - MD5å…¨å±€å»é‡
      - æ¯æ•°æ®é›†æœ€å¤šè½ç›˜ 5 ä¸ª
      - è¡¨åå¤šæ ·æ€§ä¼˜å…ˆï¼ˆname_sig ä¸åŒä¼˜å…ˆï¼‰
      - æ–‡ä»¶åä¹±ç ï¼šè½ç›˜ç»Ÿä¸€å®‰å…¨æ–‡ä»¶å + indexè®°å½•åŸå§‹/ä¿®å¤å
    """
    if len(csv_hashes) >= TARGET_MAX:
        return 0

    scanned = 0
    candidates_by_name = defaultdict(list)
    all_candidates = []

    def add_candidate(tmp_path, orig_zip_name, fixed_zip_name, rows, cols, md5, sig):
        if len(candidates_by_name[sig]) >= 20:
            try:
                os.remove(tmp_path)
            except:
                pass
            return

        cand = {
            "tmp_path": tmp_path,
            "orig_zip_name": orig_zip_name,
            "fixed_zip_name": fixed_zip_name,
            "basename": os.path.basename(fixed_zip_name),
            "rows": rows,
            "cols": cols,
            "md5": md5,
            "sig": sig
        }
        candidates_by_name[sig].append(cand)
        all_candidates.append(cand)

    try:
        with zipfile.ZipFile(zip_path, "r") as zf:
            for orig_zip_name in zf.namelist():
                if len(csv_hashes) >= TARGET_MAX:
                    break
                if scanned >= MAX_SCAN_CSV_ENTRIES_PER_DATASET:
                    break
                if not orig_zip_name.lower().endswith(".csv"):
                    continue

                scanned += 1

                fixed_zip_name = try_fix_zip_name(orig_zip_name)
                base = os.path.basename(fixed_zip_name)
                if not base:
                    continue

                tmp_path = os.path.join(CSV_DIR, f"_tmp_{time.time_ns()}.csv")
                try:
                    # è§£å‹å†…å®¹ä»ç„¶ç”¨ orig_zip_nameï¼ˆçœŸå®å­˜åœ¨çš„æ¡ç›®ï¼‰
                    with zf.open(orig_zip_name) as src, open(tmp_path, "wb") as dst:
                        dst.write(src.read())
                except Exception:
                    try:
                        os.remove(tmp_path)
                    except:
                        pass
                    continue

                try:
                    rows, cols = count_rows_cols(tmp_path)
                except Exception:
                    os.remove(tmp_path)
                    continue

                if rows < MIN_ROWS or rows > MAX_ROWS or cols < MIN_COLS:
                    os.remove(tmp_path)
                    continue

                md5 = file_hash(tmp_path)
                if md5 in csv_hashes:
                    os.remove(tmp_path)
                    continue

                sig = name_signature(base)
                add_candidate(tmp_path, orig_zip_name, fixed_zip_name, rows, cols, md5, sig)

                if len(candidates_by_name) >= MAX_CSV_PER_DATASET and len(all_candidates) >= MAX_CSV_PER_DATASET * 2:
                    break

        # é€‰æ‹©ï¼šä¼˜å…ˆä¸åŒè¡¨å
        selected = []
        selected_md5 = set()

        sigs = list(candidates_by_name.keys())
        sigs.sort(key=lambda s: len(candidates_by_name[s]), reverse=True)

        for sig in sigs:
            if len(selected) >= MAX_CSV_PER_DATASET:
                break
            cand = max(candidates_by_name[sig], key=lambda c: c["rows"])
            if cand["md5"] in selected_md5:
                continue
            selected.append(cand)
            selected_md5.add(cand["md5"])

        if len(selected) < MAX_CSV_PER_DATASET:
            remaining = sorted(all_candidates, key=lambda c: c["rows"], reverse=True)
            for cand in remaining:
                if len(selected) >= MAX_CSV_PER_DATASET:
                    break
                if cand["md5"] in selected_md5:
                    continue
                selected.append(cand)
                selected_md5.add(cand["md5"])

        selected_tmp = set(c["tmp_path"] for c in selected)

        # ç»Ÿä¸€å®‰å…¨æ–‡ä»¶å
        added = 0
        for cand in selected:
            if len(csv_hashes) >= TARGET_MAX:
                try:
                    os.remove(cand["tmp_path"])
                except:
                    pass
                continue

            safe_name = safe_output_name(cand["basename"], cand["md5"])
            final_name = safe_unique_name(safe_name)
            final_path = os.path.join(CSV_DIR, final_name)

            try:
                os.rename(cand["tmp_path"], final_path)
            except Exception:
                try:
                    os.remove(cand["tmp_path"])
                except:
                    pass
                continue

            csv_hashes.add(cand["md5"])
            index_rows.append([
                final_name,
                cand["rows"],
                cand["cols"],
                file_size_kb(final_path),
                cand["md5"],
                dataset_ref,
                keyword,
                cand["sig"],
                cand["orig_zip_name"],
                cand["fixed_zip_name"],
            ])
            added += 1

        # æ¸…ç†æœªé€‰ä¸­çš„ä¸´æ—¶æ–‡ä»¶
        for cand in all_candidates:
            if cand["tmp_path"] not in selected_tmp:
                try:
                    os.remove(cand["tmp_path"])
                except:
                    pass

        return added

    except Exception as e:
        print("âŒ è§£å‹/ç­›é€‰å¤±è´¥:", zip_path, e)
        for cand in all_candidates:
            try:
                os.remove(cand["tmp_path"])
            except:
                pass
        return 0


# ------------------------------ ä¸»æµç¨‹ ------------------------------
def main():
    print("===== Kaggle CSV Pipeline FINALï¼ˆå¸¦é‡è¯•/é™2GB/æ¯æ•°æ®é›†â‰¤5 CSVï¼‰=====")
    print("è¾“å‡ºç›®å½•ï¼š", BASE_DIR)
    print(f"é™åˆ¶ï¼šdataset<= {MAX_DATASET_TOTAL_MB}MB | rows {MIN_ROWS}-{MAX_ROWS} | cols>={MIN_COLS} | per-dataset<= {MAX_CSV_PER_DATASET}")

    for kw in SEARCH_KEYWORDS:
        for page in range(1, PAGES_PER_KEYWORD + 1):
            if len(csv_hashes) >= TARGET_MAX:
                break

            print(f"\nğŸ” æœç´¢ [{kw}] ç¬¬ {page} é¡µ")
            out = kaggle_list_datasets(kw, page)
            if out is None:
                print("âŒ æœç´¢å¤±è´¥ï¼ˆå¯èƒ½é™æµ/ç½‘ç»œæ³¢åŠ¨ï¼‰ï¼Œè·³è¿‡è¿™ä¸€é¡µ")
                continue

            lines = out.splitlines()
            if len(lines) < 3:
                continue

            for line in lines[2:]:
                if len(csv_hashes) >= TARGET_MAX:
                    break

                line = line.strip()
                if not line:
                    continue

                ref = line.split(",")[0].strip()
                if "/" not in ref or ref in downloaded_datasets:
                    continue

                print("ğŸ“ æ£€æŸ¥å¤§å°:", ref)
                total_mb = dataset_total_size_mb(ref)

                if total_mb == float("inf"):
                    if ALLOW_DOWNLOAD_IF_SIZE_UNKNOWN:
                        print("âš ï¸ å¤§å°æœªçŸ¥ï¼šå…è®¸ä¸‹è½½ï¼Œä¸‹è½½åå†æŒ‰zipå¤§å°åš2GBè¿‡æ»¤")
                        total_mb = -1.0
                    else:
                        print("â­ï¸ è·³è¿‡ï¼ˆæ— æ³•è·å–æ–‡ä»¶åˆ—è¡¨/å¤§å°ï¼‰")
                        continue

                if total_mb > MAX_DATASET_TOTAL_MB:
                    print(f"â­ï¸ è·³è¿‡ï¼ˆ{total_mb:.1f} MB > {MAX_DATASET_TOTAL_MB} MBï¼‰")
                    continue

                clear_raw_zips()

                print(f"â¬‡ï¸ ä¸‹è½½ ({total_mb:.1f} MB):", ref)
                if not kaggle_download(ref):
                    print("â­ï¸ ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡")
                    continue

                downloaded_datasets.add(ref)

                zip_path = newest_zip_in_dir(RAW_DIR)
                if not zip_path:
                    print("âš ï¸ æ²¡æ‰¾åˆ° zipï¼ˆå¯èƒ½ä¸‹è½½è¢«æ‹’ç»/å¤±è´¥ï¼‰")
                    continue

                zip_mb = os.path.getsize(zip_path) / (1024 * 1024)
                if zip_mb > MAX_DATASET_TOTAL_MB:
                    print(f"â­ï¸ zipå¤ªå¤§ï¼Œåˆ é™¤å¹¶è·³è¿‡ï¼ˆ{zip_mb:.1f} MB > {MAX_DATASET_TOTAL_MB} MBï¼‰")
                    try:
                        os.remove(zip_path)
                    except:
                        pass
                    continue

                added = extract_and_filter(zip_path, ref, kw)
                print(f"  âœ æ–°å¢ CSV: {added} | å½“å‰æ€»æ•°: {len(csv_hashes)}")

                try:
                    os.remove(zip_path)
                except:
                    pass

                write_index()
                time.sleep(BASE_SLEEP + random.random() * (JITTER_SLEEP[1] - JITTER_SLEEP[0]))

    write_index()

    if os.path.exists(RAW_DIR):
        try:
            shutil.rmtree(RAW_DIR)
            print("\nğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç›®å½• raw_datasets")
        except Exception as e:
            print("\nâš ï¸ æ¸…ç† raw_datasets å¤±è´¥:", e)

    print("\n===== Pipeline å®Œæˆ =====")
    print("æœ€ç»ˆ CSV æ•°é‡:", len(csv_hashes))
    print("CSV ç›®å½•:", CSV_DIR)
    print("ç´¢å¼•æ–‡ä»¶:", INDEX_PATH)


if __name__ == "__main__":
    main()
